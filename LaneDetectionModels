#imports
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torchvision import datasets, transforms
import torchvision
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torch.autograd import Variable
from torchvision.models import resnet50, ResNet50_Weights
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#Drive Setup
from google.colab import drive
drive.mount('/content/drive')

#Data
#Train
X_train = #Train Inputs of tvtLane as tensor
y_train = #TrainLabels of tvtLane as tensor
train_Data = torch.utils.data.TensorDataset(X_train, y_train)
#Test
X_test = #Test Inputs of tvtLane as tensor
y_test = #Test Labels of tvtLane as tensor
test_Data = torch.utils.data.TensorDataset(X_test, y_test)

#Lane Detector AutoEncoder
class LaneDetectorAE(nn.Module):
    def __init__(self):
        super(LaneDetectorAE, self).__init__()
        #Encoder
        self.Encoder = nn.Sequential(
          #Block 1
          nn.Conv2d(3, 64, 3, padding = 'same'),
          nn.BatchNorm2d(64),
          nn.ReLU(),
          nn.Conv2d(64, 64, 3, padding = 'same'),
          nn.BatchNorm2d(64),
          nn.ReLU(),
          nn.MaxPool2d(2),
          #Block 2
          nn.Conv2d(64, 128, 3, padding = 'same'),
          nn.BatchNorm2d(128),
          nn.ReLU(),
          nn.Conv2d(128, 128, 3, padding = 'same'),
          nn.BatchNorm2d(128),
          nn.ReLU(),
          nn.MaxPool2d(2),
          #Block 3
          nn.Conv2d(128, 256, 3, padding = 'same'),
          nn.BatchNorm2d(256),
          nn.ReLU(),
          nn.Conv2d(256, 256, 3, padding = 'same'),
          nn.BatchNorm2d(256),
          nn.ReLU(),
          nn.MaxPool2d(2),
          #Block 4
          nn.Conv2d(256, 512, 3, padding = 'same'),
          nn.BatchNorm2d(512),
          nn.ReLU(),
          nn.Conv2d(512, 512, 3, padding = 'same'),
          nn.BatchNorm2d(512),
          nn.ReLU(),
          nn.MaxPool2d(2),
          ).to(device)
        #Decoder
        self.Decoder = nn.Sequential(
          #Block 1
          nn.Conv2d(512, 1024, 3, padding = 'same'),
          nn.BatchNorm2d(1024),
          nn.ReLU(),
          nn.Conv2d(1024, 1024, 3, padding = 'same'),
          nn.BatchNorm2d(1024),
          nn.ReLU(),
          nn.ConvTranspose2d(1024,512,2, stride = 2),
          #Block 2
          nn.Conv2d(512, 512, 3, padding = 'same'),
          nn.BatchNorm2d(512),
          nn.ReLU(),
          nn.Conv2d(512, 512, 3, padding = 'same'),
          nn.BatchNorm2d(512),
          nn.ReLU(),
          nn.ConvTranspose2d(512,256,2, stride = 2),
          #Block 3
          nn.Conv2d(256, 256, 3, padding = 'same'),
          nn.BatchNorm2d(256),
          nn.ReLU(),
          nn.Conv2d(256, 256, 3, padding = 'same'),
          nn.BatchNorm2d(256),
          nn.ReLU(),
          nn.ConvTranspose2d(256,128,2, stride = 2),
          #Block 4
          nn.Conv2d(128, 128, 3, padding = 'same'),
          nn.BatchNorm2d(128),
          nn.ReLU(),
          nn.Conv2d(128, 128, 3, padding = 'same'),
          nn.BatchNorm2d(128),
          nn.ReLU(),
          nn.ConvTranspose2d(128,64,2, stride = 2),
          #Output
          nn.Conv2d(64, 64, 3, padding = 'same'),
          nn.BatchNorm2d(64),
          nn.ReLU(),
          nn.Conv2d(64, 64, 3, padding = 'same'),
          nn.BatchNorm2d(64),
          nn.ReLU(),
          nn.Conv2d(64, 1, 1, padding = 'same')
          ).to(device)
    #forward function
    def forward(self, x):
      x_Encoded = self.Encoder(x) #passes images through encoder
      x_Decoded = self.Decoder(x_Encoded) #passes encoded image through decoder
      return x_Decoded #returns the decoded image
LaneDetectorAE = LaneDetectorAE()

#UNet Lane Detector

#ConvBlock Class
class ConvBlock(nn.Module):
  def __init__(self, input, output, kernel_size=3):
    super(ConvBlock, self).__init__()
    self.ConvBlock = nn.Sequential(
          nn.Conv2d(input, output, 1, padding = 'same'),
          nn.BatchNorm2d(output),
          nn.ReLU(),
          nn.Conv2d(output, output, kernel_size, padding = 'same'),
          nn.BatchNorm2d(output),
          nn.ReLU()
        )
  def forward(self, x):
    x = self.ConvBlock(x)
    return x

#UpConv Class
class UpConv(nn.Module):
  def __init__(self, input, output):
    super(UpConv, self).__init__()
    self.UpConv = nn.ConvTranspose2d(input,output,2, stride = 2)
  def forward(self, x):
    x = self.UpConv(x)
    return x

class LaneDetectorU(nn.Module):
  def __init__(self):
        super(LaneDetectorU, self).__init__()
        #Encoder
        self.EncBlock_1 = ConvBlock(3, 64).to(device)         #Block 1
        self.EncBlock_2 = ConvBlock(64,128).to(device)        #Block 2
        self.EncBlock_3 = ConvBlock(128, 256).to(device)      #Block 3
        self.EncBlock_4 = ConvBlock(256,512).to(device)       #Block 4
        self.MaxPool2d = nn.MaxPool2d(2).to(device)           #MaxPooling
        #Bottleneck
        self.Bottleneck = ConvBlock(512, 1024).to(device)
        #Decoder
        self.UpConv_1 = UpConv(1024, 512).to(device)          #UpConv 1
        self.DecBlock_1 = ConvBlock(1024, 512).to(device)     #Block 1
        self.UpConv_2 = UpConv(512, 256).to(device)           #UpConv 2
        self.DecBlock_2 = ConvBlock(512, 256).to(device)      #Block 2
        self.UpConv_3 = UpConv(256, 128).to(device)           #UpConv 3
        self.DecBlock_3 = ConvBlock(256, 128).to(device)      #Block 3
        self.UpConv_4 = UpConv(128, 64).to(device)            #UpConv 4
        self.DecBlock_4 = ConvBlock(128, 64).to(device)      #Block 4
        self.DecBlock_5 = ConvBlock(64, 64).to(device)        #Block 5
        #Output
        self.Out = nn.Sequential(
            nn.Conv2d(64, 1, 1, padding = 'same')
        ).to(device)
  def forward(self, x):
      #Encoder
      #Block 1
      e1 = self.EncBlock_1(x)
      p1 = self.MaxPool2d(e1)
      #Block 2
      e2 = self.EncBlock_2(p1)
      p2 = self.MaxPool2d(e2)
      #Block 3
      e3 = self.EncBlock_3(p2)
      p3 = self.MaxPool2d(e3)
      #Block 4
      e4 = self.EncBlock_4(p3)
      p4 = self.MaxPool2d(e4)

      #Bottleneck
      BotNeck = self.Bottleneck(p4)

      #Decoder
      #Block 1
      uc1 = self.UpConv_1(BotNeck)
      d1 = torch.cat([uc1, e4],dim=1)
      d1 = self.DecBlock_1(d1)
      #Block 2
      uc2 = self.UpConv_2(d1)
      d2 = torch.cat([uc2, e3],dim=1)
      d2 = self.DecBlock_2(d2)
      #Block 3
      uc3 = self.UpConv_3(d2)
      d3 = torch.cat([uc3, e2], dim=1)
      d3 = self.DecBlock_3(d3)
      #Block 4
      uc4 = self.UpConv_4(d3)
      d4 = torch.cat([uc4, e1], dim=1)
      d4 = self.DecBlock_4(d4)

      #Output
      output = self.DecBlock_5(d4)
      output = self.Out(output)
      return output

LaneDetectorU = LaneDetectorU()

#U-Net with LSTM

#ConvBlock Class
class ConvBlock(nn.Module):
  def __init__(self, input, output, kernel_size=3):
    super(ConvBlock, self).__init__()
    self.ConvBlock = nn.Sequential(
          nn.Conv2d(input, output, 1, padding = 'same'),
          nn.BatchNorm2d(output),
          nn.ReLU(),
          nn.Conv2d(output, output, kernel_size, padding = 'same'),
          nn.BatchNorm2d(output),
          nn.ReLU()
        )
  def forward(self, x):
    x = self.ConvBlock(x)
    return x

#UpConv Class
class UpConv(nn.Module):
  def __init__(self, input, output):
    super(UpConv, self).__init__()
    self.UpConv = nn.ConvTranspose2d(input,output,2, stride = 2)
  def forward(self, x):
    x = self.UpConv(x)
    return x

#LSTM Class
class LSTM_Cell(nn.Module):
  def __init__(self, input_1, input_2, output):
    super(LSTM_Cell, self).__init__()
    self.Conv1 = nn.Sequential(
        nn.Conv2d(input_1, output, kernel_size=3, padding = 'same'),
        nn.BatchNorm2d(512),
        nn.ReLU()
    )
    self.Conv2 = nn.Sequential(
        nn.Conv2d(input_2, output, kernel_size=3, padding = 'same'),
        nn.BatchNorm2d(512),
        nn.ReLU()
    )
    self.BN = nn.BatchNorm2d(1024)
    self.f = nn.Sigmoid()
    self.i = nn.Sigmoid()
    self.c = nn.Tanh()
    self.o = nn.Sigmoid()
    self.out = nn.Tanh()

  def forward(self, x, hidden_state, current_state):
    x_conv = self.Conv1(x)
    hidden_conv = self.Conv2(hidden_state)
    x_combined = torch.cat((x_conv, hidden_conv), dim= 1)
    f = self.f(x_combined)
    i = self.i(x_combined)
    c = self.c(x_combined)
    o = self.o(x_combined)
    current_state_next = self.BN((f*current_state) + (i*c))
    hidden_state_next = o * self.out(current_state_next)
    return hidden_state_next, current_state_next

class LaneDetectorULSTM(nn.Module):
  def __init__(self):
        super(LaneDetectorULSTM, self).__init__()
        #Encoder
        self.EncBlock_1 = ConvBlock(3, 64).to(device)         #Block 1
        self.EncBlock_2 = ConvBlock(64,128).to(device)        #Block 2
        self.EncBlock_3 = ConvBlock(128, 256).to(device)      #Block 3
        self.EncBlock_4 = ConvBlock(256,512).to(device)       #Block 4
        self.MaxPool2d = nn.MaxPool2d(2).to(device)           #MaxPooling
        #Bottleneck
        self.LSTM_Cell_1 = LSTM_Cell(512, 1024, 512).to(device)
        self.LSTM_Cell_2 = LSTM_Cell(1024, 1024, 512).to(device)
        #self.LSTM_Cell_3 = LSTM_Cell(1024, 1024, 512).to(device)
        #Decoder
        self.UpConv_1 = UpConv(1024, 512).to(device)          #UpConv 1
        self.DecBlock_1 = ConvBlock(1024, 512).to(device)     #Block 1
        self.UpConv_2 = UpConv(512, 256).to(device)           #UpConv 2
        self.DecBlock_2 = ConvBlock(512, 256).to(device)      #Block 2
        self.UpConv_3 = UpConv(256, 128).to(device)           #UpConv 3
        self.DecBlock_3 = ConvBlock(256, 128).to(device)      #Block 3
        self.UpConv_4 = UpConv(128, 64).to(device)            #UpConv 4
        self.DecBlock_4 = ConvBlock(128, 64).to(device)      #Block 4
        self.DecBlock_5 = ConvBlock(64, 64).to(device)        #Block 5
        #Output
        self.Out = nn.Sequential(
            nn.Conv2d(64, 1, 1, padding = 'same')
        ).to(device)

  def forward(self, x):
      #Encoder
      #Block 1
      e1 = self.EncBlock_1(x)
      p1 = self.MaxPool2d(e1)
      #Block 2
      e2 = self.EncBlock_2(p1)
      p2 = self.MaxPool2d(e2)
      #Block 3
      e3 = self.EncBlock_3(p2)
      p3 = self.MaxPool2d(e3)
      #Block 4
      e4 = self.EncBlock_4(p3)
      p4 = self.MaxPool2d(e4)

      #Bottleneck
      hidden_state_1 = torch.zeros(1,1024,8,16).to(device)
      current_state_1 = torch.zeros(1,1024,8,16).to(device)
      hidden_state_2 = torch.zeros(1,1024,8,16).to(device)
      current_state_2 = torch.zeros(1,1024,8,16).to(device)
      BottleNeck = torch.zeros(5,1024,8,16).to(device)
      for t in range(5):
        x_t = p4[t]
        x_t = x_t.reshape(1,512,8,16)
        hidden_state_1, current_state_1 = self.LSTM_Cell_1(x_t, hidden_state_1, current_state_1)
        #hiden_state_2, current_state_2 = self.LSTM_Cell_2(hidden_state_1, hidden_state_2, current_state_2)
        BottleNeck[t] = hidden_state_1

      #Decoder
      #Block 1
      uc1 = self.UpConv_1(BottleNeck)
      d1 = torch.cat([uc1, e4],dim=1)
      d1 = self.DecBlock_1(d1)
      #Block 2
      uc2 = self.UpConv_2(d1)
      d2 = torch.cat([uc2, e3],dim=1)
      d2 = self.DecBlock_2(d2)
      #Block 3
      uc3 = self.UpConv_3(d2)
      d3 = torch.cat([uc3, e2], dim=1)
      d3 = self.DecBlock_3(d3)
      #Block 4
      uc4 = self.UpConv_4(d3)
      d4 = torch.cat([uc4, e1], dim=1)
      d4 = self.DecBlock_4(d4)

      #Output
      output = self.DecBlock_5(d4)
      output = self.Out(output)
      return output

LaneDetectorULSTM = LaneDetectorULSTM()

#U-Former

#ConvBlock Class
class ConvBlock(nn.Module):
  def __init__(self, input, output, kernel_size=3):
    super(ConvBlock, self).__init__()
    self.ConvBlock = nn.Sequential(
          nn.Conv2d(input, output, 1, padding = 'same'),
          nn.BatchNorm2d(output),
          nn.ReLU(),
          nn.Conv2d(output, output, 3, padding = 'same'),
          nn.BatchNorm2d(output),
          nn.ReLU()
        )
  def forward(self, x):
    x = self.ConvBlock(x)
    return x

#Depthwise Sep with Linear BN
class DpthSepWLinearBt(nn.Module):
  def __init__(self, input,BottleNeck, output):
    super(DpthSepWLinearBt, self).__init__()
    self.DepthWise = nn.Sequential(
        nn.Conv2d(input,input,3, padding=1, groups=input),
        nn.BatchNorm2d(input),
        nn.ReLU()
    )
    self.PointWise = nn.Sequential(
        nn.Conv2d(input,BottleNeck,1),
        nn.BatchNorm2d(BottleNeck),
        nn.ReLU()
    )
    self.LinearBottleneck = nn.Sequential(
        nn.Conv2d(BottleNeck, output, 1),
        nn.BatchNorm2d(output)
    )
  def forward(self, x):
    x = self.DepthWise(x)
    x = self.PointWise(x)
    x = self.LinearBottleneck(x)
    return x

#Attention
class AttnConvBlock(nn.Module):
  def __init__(self, input, output):
    super(AttnConvBlock, self).__init__()
    self.Conv1 = nn.Sequential(
        nn.Conv2d(input, int(input/4), 1, padding = 'same'),
        nn.BatchNorm2d(int(input/4)),
    )
    self.Conv2 = nn.Sequential(
        nn.Conv2d(input, int(input/4), 1, padding = 'same'),
        nn.BatchNorm2d(int(input/4)),
    )
    self.Conv3 = nn.Sequential(
        nn.Conv2d(input, int(input/4), 1, padding = 'same'),
        nn.BatchNorm2d(int(input/4)),
    )
    self.Conv4 = nn.Sequential(
        nn.Conv2d(input, int(input/4), 1, padding = 'same'),
        nn.BatchNorm2d(int(input/4)),
    )
    self.Conv9 = nn.Sequential(
        nn.Conv2d(input, int(input/4), 1, padding = 'same'),
        nn.BatchNorm2d(int(input/4)),
    )
    self.Conv10 = nn.Sequential(
        nn.Conv2d(input, int(input/4), 1, padding = 'same'),
        nn.BatchNorm2d(int(input/4)),
    )
    self.Conv11 = nn.Sequential(
        nn.Conv2d(input, int(input/4), 1, padding = 'same'),
        nn.BatchNorm2d(int(input/4)),
    )
    self.Conv12 = nn.Sequential(
        nn.Conv2d(input, int(input/4), 1, padding = 'same'),
        nn.BatchNorm2d(int(input/4)),
    )
    self.Softmax1 = nn.Softmax(dim=2)
    self.Softmax2 = nn.Softmax(dim=2)
    self.Softmax3 = nn.Softmax(dim=2)
    self.Softmax4 = nn.Softmax(dim=2)
    self.Output = DpthSepWLinearBt(input, int(output/4), output)
    self.ResConv = nn.Sequential(
        nn.Conv2d(input, output, 1, padding = 'same'),
        nn.BatchNorm2d(output),
    )
    self.ReLU = nn.ReLU()
  def forward(self, x, Prev_Output, j):
    Q1 = self.Conv1(x)
    Q2 = self.Conv2(x)
    Q3 = self.Conv3(x)
    Q4 = self.Conv4(x)
    if j == 0:
        V1 = self.Conv1(x)
        V2 = self.Conv2(x)
        V3 = self.Conv3(x)
        V4 = self.Conv4(x)
    else:
        V1 = self.Conv9(Prev_Output)
        V2 = self.Conv10(Prev_Output)
        V3 = self.Conv11(Prev_Output)
        V4 = self.Conv12(Prev_Output)
    H1 = self.Softmax1(Q1) * V1
    H2 = self.Softmax2(Q2) * V2
    H3 = self.Softmax3(Q3) * V3
    H4 = self.Softmax4(Q4) * V4
    if Prev_Output in locals():
      if j == 0:
        Attention = torch.cat((H1,H2,H3,H4),dim=1) + x
      else:
        Attention = torch.cat((H1,H2,H3,H4),dim=1) + Prev_Output
    else:
      Attention = torch.cat((H1,H2,H3,H4),dim=1) + x
    Output = self.Output(Attention) + self.ResConv(Attention)
    return Output

#DownConv Class
class DownConv(nn.Module):
  def __init__(self, input, output):
    super(DownConv, self).__init__()
    self.DownConv = nn.Conv2d(input, output, 2, stride = 2)
  def forward(self,x):
    x = self.DownConv(x)
    return x

#UpConv Class
class UpConv(nn.Module):
  def __init__(self, input, output):
    super(UpConv, self).__init__()
    self.UpConv = nn.ConvTranspose2d(input,output,2, stride = 2)
  def forward(self, x):
    x = self.UpConv(x)
    return x


class LaneDetectorU(nn.Module):
  def __init__(self):
        super(LaneDetectorU, self).__init__()
        self.PrevOutConv = nn.Sequential(
            nn.Conv2d(1,64,7, padding='same'),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2),
            DpthSepWLinearBt(64, 16, 128),
            nn.MaxPool2d(2),
            DpthSepWLinearBt(128, 32, 256),
            nn.MaxPool2d(2),
            DpthSepWLinearBt(256, 64, 512),
            nn.MaxPool2d(2)
        ).to(device)
        #Encoder
        self.EncBlock_1 = ConvBlock(3, 64).to(device)         #Block 1
        #self.DownConv_1 = DownConv(64, 64).to(device)           #DownConv 1
        self.EncBlock_2 = ConvBlock(64,128).to(device)        #Block 2
        #self.DownConv_2 = DownConv(128, 128).to(device)           #DownConv 2
        self.EncBlock_3 = ConvBlock(128, 256).to(device)      #Block 3
        #self.DownConv_3 = DownConv(256, 256).to(device)           #DownConv 3
        self.EncBlock_4 = ConvBlock(256,512).to(device)       #Block 4
        #self.DownConv_4 = DownConv(512, 512).to(device)           #DownConv 4
        self.MaxPool = nn.MaxPool2d(2)
        #Bottleneck
        self.Bottleneck = AttnConvBlock(512, 1024).to(device)
        #Decoder
        self.UpConv_1 = UpConv(1024, 512).to(device)          #UpConv 1
        self.DecBlock_1 = ConvBlock(1024, 512).to(device)     #Block 1
        self.UpConv_2 = UpConv(512, 256).to(device)           #UpConv 2
        self.DecBlock_2 = ConvBlock(512, 256).to(device)      #Block 2
        self.UpConv_3 = UpConv(256, 128).to(device)           #UpConv 3
        self.DecBlock_3 = ConvBlock(256, 128).to(device)      #Block 3
        self.UpConv_4 = UpConv(128, 64).to(device)            #UpConv 4
        self.DecBlock_4 = ConvBlock(128, 64).to(device)      #Block 4
        self.DecBlock_5 = ConvBlock(64, 64).to(device)        #Block 5
        #Output
        self.Out = nn.Sequential(
            nn.Conv2d(64, 1, 1, padding = 'same'),
        ).to(device)
  def forward(self, x):
      #Encoder
      #Block 1
      e1 = self.EncBlock_1(x)
      p1 = self.MaxPool(e1)
      #Block 2
      e2 = self.EncBlock_2(p1)
      p2 = self.MaxPool(e2)
      #Block 3
      e3 = self.EncBlock_3(p2)
      p3 = self.MaxPool(e3)
      #Block 4
      e4 = self.EncBlock_4(p3)
      p4 = self.MaxPool(e4)

      #Encode Output
      prev_out = torch.zeros(1,1,128,256).to(device)
      output_full = torch.zeros(5,1,128,256).to(device)
      for j in range(5):
        if j == 0:
          Conv_prev_out = 0
        else:
          Conv_prev_out = self.PrevOutConv(prev_out)
        input = p4[j].reshape(1,512,8,16)
        #Bottleneck
        BotNeck = self.Bottleneck(input,Conv_prev_out,j)
        #Decoder
        #Block 1
        uc1 = self.UpConv_1(BotNeck)
        d1 = torch.cat([uc1, torch.unsqueeze(e4[j],dim=0)],dim=1)
        d1 = self.DecBlock_1(d1)
        #Block 2
        uc2 = self.UpConv_2(d1)
        d2 = torch.cat([uc2, torch.unsqueeze(e3[j],dim=0)],dim=1)
        d2 = self.DecBlock_2(d2)
        #Block 3
        uc3 = self.UpConv_3(d2)
        d3 = torch.cat([uc3, torch.unsqueeze(e2[j],dim=0)], dim=1)
        d3 = self.DecBlock_3(d3)
        #Block 4
        uc4 = self.UpConv_4(d3)
        d4 = torch.cat([uc4, torch.unsqueeze(e1[j],dim=0)], dim=1)
        d4 = self.DecBlock_4(d4)

        #Output
        output = self.DecBlock_5(d4)
        output = self.Out(output)
        # if j == 0:
        #   prev_out[0][0] = output
        #   prev_out[0][1] = output
        #   prev_out[0][2] = output
        #   prev_out[0][3] = output
        #   prev_out[0][4] = output
        # else:
        #   prev_out[0][j] = output
        prev_out = output
        output_full[j] = output
      return output_full

UFormer = LaneDetectorU()

#Loss Function
def weighted_BCE(y_pred, y_true, pos_weight):
    #Flatten
    y_pred_flat = y_pred.view(-1)
    y_true_flat = y_true.view(-1)
    # Define the binary cross-entropy loss function with logits
    bce_logits_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

    # Calculate the loss
    loss = bce_logits_loss(y_pred_flat, y_true_flat)

    return loss

#Accuracy and F1
#F1 Score Test Metric
def f1_score(y_pred, y_true, epsilon=1e-7):
    # Flatten the tensors
    y_pred_flat = y_pred.view(-1)
    y_true_flat = y_true.view(-1)

    # Convert to binary values (0 or 1) based on a threshold (e.g., 0.5)
    y_pred_flat = (y_pred_flat > 0.5).float()
    #y_true_flat = (y_true_flat > 0.5).float()

    # True Positives, False Positives, False Negatives
    tp = torch.sum((y_true_flat == 1) & (y_pred_flat == 1)).float()
    fp = torch.sum((y_true_flat == 0) & (y_pred_flat == 1)).float()
    fn = torch.sum((y_true_flat == 1) & (y_pred_flat == 0)).float()

    # Calculate precision and recall
    precision = tp / (tp + fp + epsilon)
    recall = tp / (tp + fn + epsilon)

    # Calculate F1 score
    f1 = 2 * (precision * recall) / (precision + recall + epsilon)

    return f1
def accuracy(y_pred,y_true):
    # Flatten the tensors
    y_pred_flat = y_pred.view(-1)
    y_true_flat = y_true.view(-1)
    # Convert to binary values (0 or 1) based on a threshold (e.g., 0.5)
    y_pred_flat = (y_pred_flat > 0.5).float()
    #Get accuracy
    A = torch.sum(y_pred_flat == y_true_flat)
    accuracy = A/32768
    return accuracy

# Train Scheduler
best_metric = 100
opt = optim.Adam(LaneDetectorULSTM.parameters(), lr = 0.01)
from torch.optim.lr_scheduler import ReduceLROnPlateau
factor = 0.5     # Factor by which the learning rate will be decreased (e.g., 0.5 means halving the learning rate)
patience = 2     # Number of epochs with no improvement after which learning rate will be reduced
min_lr = 1e-6    # Lower bound on the learning rate
scheduler = ReduceLROnPlateau(opt, mode='min', factor=factor, patience=patience, min_lr=min_lr)

#Training
def epoch(model, loader, opt=None):
  #i = 0
  total_loss = 0.
  total_F1 = 0.
  #Load data
  for X,y in loader:
    X,y = X.to(device), (y > 0.0).to(torch.float32).to(device)
    #Get model output
    yp = model(X[0])[4
    #Get loss
    loss = weighted_BCE(yp,y,torch.tensor(5))
    F1 = f1_score(yp, y)
    total_loss += loss.item() * X.shape[0]
    total_F1 += F1.item() * X.shape[0]
    #Optimize
    if opt:
      opt.zero_grad()
      loss.backward()
      opt.step()
  #End Epoch
  return total_loss / (len(loader.dataset)), total_F1 / (len(loader.dataset))
def Test_epoch(model, loader):
  total_loss = 0.
  total_F1 = 0.
  #Load data
  for X,y in loader:
    X,y = X.to(device), (y > 0.0).to(torch.float32).to(device)
    #Get model output
    yp = model(X[0])[4]
    #Get loss
    loss = weighted_BCE(yp,y,torch.tensor(5))
    F1 = f1_score(yp, y)
    #F1 = accuracy(yp,y)
    total_loss += loss.item() * X.shape[0]
    total_F1 += F1.item() * X.shape[0]
  #End Epoch
  return total_loss / (len(loader.dataset)), total_F1/(len(loader.dataset))
#Training
i = 0
for _ in range(50):
  i += 1
  print('Epoch: %s' %i)
  #Train
  print("Train")
  train_loss, train_F1 = epoch(LaneDetectorULSTM, train_loader, opt)
  print('Train Loss: %s' %train_loss)
  print('Train F1: %s' %train_F1)
  #Small Test
  print("Test")
  test_loss,test_F1 = Test_epoch(LaneDetectorULSTM, test_loader)
  print('Test Loss: %s' %test_loss)
  print('Test F1: %s' %test_F1)
  # #If test loss isnt improving after 3 epochs lower the lr by 1/2
  scheduler.step(test_loss)
  #If test loss improves save weights as checkpoint, and make new best metric
  if test_loss < best_metric:
    best_metric = test_loss
    torch.save(LaneDetectorVULSTM.state_dict(), '/content/drive/My Drive/CATVehicle/Model_Params/UVLSTM_checkpt')
  #If test loss didn't improve revert to previous weights
  else:
    LaneDetectorVULSTM.load_state_dict(torch.load('/content/drive/My Drive/CATVehicle/Model_Params/UVLSTM_checkpt'))
